# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import torch
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.retrievers import BM25Retriever
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import gradio as gr

# 1. pdfplumberë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° Document ê°ì²´ë¡œ ë³€í™˜
pdf_text = extract_text_from_pdf("í•œí™”ìƒëª… ê°„í¸ê°€ì… ì‹œê·¸ë‹ˆì²˜ ì•”ë³´í—˜(ê°±ì‹ í˜•) ë¬´ë°°ë‹¹_2055-001_002_ì•½ê´€_20220601_(2).pdf")
documents = [Document(page_content=pdf_text)]

# ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í¬ ë‚˜ëˆ„ê¸°
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)

# 2. BM25 Retriever ì„¤ì •
bm25_retriever = BM25Retriever.from_documents(chunks)

# 3. LLMê³¼ ê²°í•©í•˜ì—¬ ë‹µë³€ ìƒì„±
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained("beomi/Llama-3-Open-Ko-8B")

# ëª¨ë¸ ë©”ëª¨ë¦¬ ì„¤ì • ë° ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "beomi/Llama-3-Open-Ko-8B",
    device_map="auto",
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16  # fp16ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆê°
)

# QA íŒŒì´í”„ë¼ì¸ ìƒì„±
qa_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")

# ìš”ì•½ íŒŒì´í”„ë¼ì¸ ìƒì„±
summarization_pipeline = pipeline("summarization", model="hyunwoongko/kobart")

# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def query_bm25(query):
    # BM25ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ìƒìœ„ 3ê°œ ê²°ê³¼ ì‚¬ìš©)
    try:
        results = bm25_retriever.invoke(query)[:3]
    except Exception as e:
        print(f"An error occurred while retrieving documents: {e}")
        return "Error occurred during document retrieval."
    
    if not results:
        return "No relevant documents found."
    
    summaries = []
    for doc in results:
        try:
            content = doc.page_content.strip()[:512] if doc.page_content else ""
            if len(content) > 10:
                summary = summarization_pipeline(content, max_length=100, min_length=30, do_sample=True)[0]['summary_text']
                summaries.append(summary)
            else:
                print("Document content is too short or empty.")
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"An error occurred while summarizing: {e}")
            continue
    
    if not summaries:
        return "Summarization failed for all documents."
    
    context = " ".join(summaries)
    input_text = f"{context}"
    
    # LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    llm_response = qa_pipeline(input_text, max_new_tokens=100)[0]['generated_text']
    torch.cuda.empty_cache()
    
    return llm_response

# 5. Gradio Blocks ì¸í„°í˜ì´ìŠ¤ ìƒì„±
with gr.Blocks() as iface:
    gr.Markdown("# ë³´í—˜ ë¬¸ì„œ ì±—ë´‡\në³´í—˜ ë¬¸ì„œì— ëŒ€í•´ ë¬¼ì–´ë³´ë©´ ë‹µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.")
    
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="ë­ë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”.", label="ì±— ì…ë ¥")
    
    with gr.Row():
        submit_btn = gr.Button("ë³´ë‚´ê¸°")
        retry_btn = gr.Button("ë‹¤ì‹œë³´ë‚´ê¸° â†©")
        undo_btn = gr.Button("ì´ì „ì±— ì‚­ì œ âŒ")
        clear_btn = gr.Button("ì „ì±— ì‚­ì œ ğŸ’«")
    
    # ì±„íŒ… ì œì¶œ ì‹œ ì‘ë‹µ ìƒì„±
    def submit(message, history):
        response_message = query_bm25(message)  # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        history.append((message, response_message))  # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        return history, ""

    # ë²„íŠ¼ ê¸°ëŠ¥ ì—°ê²°
    submit_btn.click(submit, [msg, chatbot], [chatbot, msg])
    retry_btn.click(lambda: None, None, chatbot)  # ë‹¤ì‹œ ë³´ë‚´ê¸° ê¸°ëŠ¥ êµ¬í˜„ í•„ìš”
    undo_btn.click(lambda history: history[:-1], [chatbot], chatbot)  # ì´ì „ ë©”ì‹œì§€ ì‚­ì œ
    clear_btn.click(lambda: [], None, chatbot)  # ì „ì²´ ë©”ì‹œì§€ ì‚­ì œ

iface.launch()
